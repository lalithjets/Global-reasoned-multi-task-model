{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Project         : Incremental learning for surgical instrument classification and feature extraction\n",
    "Lab             : MMLAB, National University of Singapore\n",
    "contributors    : Mobarak, lalith, mengya\n",
    "Note            : Dataloader for End-to-End incremental learning, code adopted from our previous work.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import xml.etree.cElementTree as ET\n",
    "else:\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "class SurgicalClassDataset18_incremental(Dataset):\n",
    "    def __init__(self, filenames, fine_tune_size = None, transform=None, is_train=None):\n",
    "        \n",
    "        self.is_train = is_train\n",
    "        self.img_list = []\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        # Using readlines() \n",
    "        for i, txt_file in enumerate(filenames):\n",
    "            curr_file = open((txt_file), 'r') \n",
    "            Lines = curr_file.readlines()  \n",
    "            if (fine_tune_size is not None) and (i == len(filenames)-1):\n",
    "                indices = np.random.permutation(len(Lines))\n",
    "                Lines = [Lines[i] for i in indices[0:fine_tune_size]]\n",
    "            for line in Lines: self.img_list.append(line. rstrip())\n",
    "            #print(self.img_list)\n",
    "            curr_file.close()\n",
    "        \n",
    "    def __len__(self): return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _img_dir = self.img_list[index]\n",
    "        _img = Image.open(_img_dir).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            _img = self.transform(_img)\n",
    "        else:\n",
    "            _img = np.asarray(_img, np.float32) / 255\n",
    "            _img = torch.from_numpy(np.array(_img).transpose(2, 0, 1,)).float()\n",
    "        \n",
    "        _target = int(_img_dir[:-4].split('_')[-1:][0])\n",
    "        _target = torch.from_numpy(np.array(_target)).long()\n",
    "        return _img, _target\n",
    "    \n",
    "class TwoCropTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transform(x), self.transform(x)] # return a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBS filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gaussian and laplacian filters for curicullum learning\n",
    "'''\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_gaussian_filter(kernel_size=3, sigma=2, channels=3):\n",
    "    '''\n",
    "    Gaussian 2D filter\n",
    "    '''\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "\n",
    "    mean = (kernel_size - 1)/2.\n",
    "    variance = sigma**2.\n",
    "\n",
    "    # Calculate the 2-dimensional gaussian kernel which is the product of two gaussian distributions \n",
    "    # for two different variables (in this case called x and y)\n",
    "    gaussian_kernel = (1./(2.*math.pi*variance)) *\\\n",
    "                      torch.exp( -torch.sum((xy_grid - mean)**2., dim=-1) / (2*variance))\n",
    "\n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "\n",
    "    # Reshape to 2d depthwise convolutional weight\n",
    "    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)\n",
    "    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1, 1)\n",
    "\n",
    "    if kernel_size == 3: padding = 1\n",
    "    elif kernel_size == 5: padding = 2\n",
    "    else: padding = 0\n",
    "\n",
    "    gaussian_filter = nn.Conv2d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels,\n",
    "                                bias=False, padding=padding)\n",
    "\n",
    "    gaussian_filter.weight.data = gaussian_kernel\n",
    "    gaussian_filter.weight.requires_grad = False\n",
    "    \n",
    "    return gaussian_filter\n",
    "\n",
    "\n",
    "def get_laplaceOfGaussian_filter(kernel_size=3, sigma=2, channels=3):\n",
    "    '''\n",
    "    laplacian 2D filter\n",
    "    '''\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "    mean = (kernel_size - 1)/2.\n",
    "\n",
    "    used_sigma = sigma\n",
    "    # Calculate the 2-dimensional gaussian kernel which is\n",
    "    log_kernel = (-1./(math.pi*(used_sigma**4))) \\\n",
    "                        * (1-(torch.sum((xy_grid - mean)**2., dim=-1) / (2*(used_sigma**2)))) \\\n",
    "                        * torch.exp(-torch.sum((xy_grid - mean)**2., dim=-1) / (2*(used_sigma**2)))\n",
    "       \n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    log_kernel = log_kernel / torch.sum(log_kernel)\n",
    "\n",
    "    # Reshape to 2d depthwise convolutional weight\n",
    "    log_kernel = log_kernel.view(1, 1, kernel_size, kernel_size)\n",
    "    log_kernel = log_kernel.repeat(channels, 1, 1, 1)\n",
    "\n",
    "    if kernel_size == 3: padding = 1\n",
    "    elif kernel_size == 5: padding = 2\n",
    "    else: padding = 0\n",
    "\n",
    "    log_filter = nn.Conv2d( in_channels=channels, out_channels=channels, kernel_size=kernel_size, \n",
    "                            groups=channels, bias=False, padding=padding)\n",
    "\n",
    "    log_filter.weight.data = log_kernel\n",
    "    log_filter.weight.requires_grad = False\n",
    "    \n",
    "    return log_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ResNet (Pytorch implementation), together with curricullum learning filters\n",
    "    Reference:\n",
    "    [1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.planes = planes\n",
    "        self.enable_cbs = False\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut_kernel = True\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def get_new_kernels(self, fil2, fil3, kernel_size, std):\n",
    "        self.enable_cbs = True\n",
    "        if (fil2 == 'gau'): \n",
    "            self.kernel1 = get_gaussian_filter(kernel_size=kernel_size, sigma= std, channels=self.planes)\n",
    "        elif (fil2 == 'LOG'): \n",
    "            self.kernel1 = get_laplaceOfGaussian_filter(kernel_size=kernel_size, sigma= std, channels=self.planes)\n",
    "\n",
    "        if (fil3 == 'gau'): \n",
    "            self.kernel2 = get_gaussian_filter(kernel_size=kernel_size, sigma= std, channels=self.planes)\n",
    "        elif (fil3 == 'LOG'): \n",
    "            self.kernel2 = get_laplaceOfGaussian_filter(kernel_size=kernel_size, sigma= std, channels=self.planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        \n",
    "        if self.enable_cbs: out = F.relu(self.bn1(self.kernel1(out)))         \n",
    "        else: out = F.relu(self.bn1(out))         \n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if self.enable_cbs: out = self.bn2(self.kernel2(out))\n",
    "        else: out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, args):\n",
    "               \n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        \n",
    "        # CBS\n",
    "        self.enable_cbs = args.use_cbs\n",
    "        self.std = args.std\n",
    "        self.factor = args.std_factor\n",
    "        self.epoch = args.cbs_epoch\n",
    "        self.kernel_size = args.kernel_size\n",
    "        self.fil1 = args.fil1\n",
    "        self.fil2 = args.fil2\n",
    "        self.fil3 = args.fil3\n",
    "\n",
    "        # Super contrast\n",
    "        self.enable_SC = args.use_SC\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        if not self.enable_SC:\n",
    "            self.linear = nn.Linear(512*block.expansion, args.num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        if self.enable_cbs: out = F.relu(self.bn1(self.kernel1(out)))\n",
    "        else: out = F.relu(self.bn1(out))\n",
    "            \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        if not self.enable_SC:\n",
    "            out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def get_new_kernels(self, epoch_count):\n",
    "        if epoch_count % self.epoch == 0 and epoch_count is not 0:\n",
    "            self.std *= self.factor\n",
    "        if (self.fil1 == 'gau'): \n",
    "            self.kernel1 = get_gaussian_filter(kernel_size=self.kernel_size, sigma= self.std, channels=64)\n",
    "        elif (self.fil1 == 'LOG'): \n",
    "            self.kernel1 = get_laplaceOfGaussian_filter(kernel_size=self.kernel_size, sigma= self.std, channels=64)\n",
    "\n",
    "        for child in self.layer1.children():\n",
    "            child.get_new_kernels(self.fil2, self.fil3, self.kernel_size, self.std)\n",
    "\n",
    "        for child in self.layer2.children():\n",
    "            child.get_new_kernels(self.fil2, self.fil3, self.kernel_size, self.std)\n",
    "\n",
    "        for child in self.layer3.children():\n",
    "            child.get_new_kernels(self.fil2, self.fil3, self.kernel_size, self.std)\n",
    "\n",
    "        for child in self.layer4.children():\n",
    "            child.get_new_kernels(self.fil2, self.fil3, self.kernel_size, self.std)\n",
    "\n",
    "\n",
    "\n",
    "def ResNet18(args): return ResNet(BasicBlock, [2,2,2,2], args)\n",
    "\n",
    "model_dict = {\n",
    "    'resnet18': [ResNet18, 512],\n",
    "    #'resnet34': [resnet34, 512],\n",
    "    #'resnet50': [resnet50, 2048],\n",
    "    #'resnet101': [resnet101, 2048],\n",
    "}\n",
    "\n",
    "class SupConResNet(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    def __init__(self, args, name='resnet18', head='mlp', feat_dim=128):\n",
    "        super(SupConResNet, self).__init__()\n",
    "        enc_model, dim_in = model_dict[name]\n",
    "        self.encoder = enc_model(args)\n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(dim_in, feat_dim)\n",
    "        elif head == 'mlp':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_in, feat_dim)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'head not supported: {}'.format(head))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        feat = F.normalize(self.head(feat), dim=1)\n",
    "        return feat\n",
    "    \n",
    "# def ResNet34(args): return ResNet(BasicBlock, [3,4,6,3], args)\n",
    "# def ResNet50(args): return ResNet(Bottleneck, [3,4,6,3], args)\n",
    "# def ResNet101(args):return ResNet(Bottleneck, [3,4,23,3], args)\n",
    "\n",
    "# def test():\n",
    "#     net = ResNet18()\n",
    "#     y = net(torch.randn(1,3,32,32))\n",
    "#     print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def test(args, model, test_loader, class_old, class_novel):\n",
    "    '''\n",
    "    arguments: net, test_loader, class_old, class_novel\n",
    "    return: tcost, acc_avg\n",
    "    '''\n",
    "\n",
    "    acc_avg = 0\n",
    "    num_exp = 0\n",
    "    tstart = time.clock()\n",
    "\n",
    "    # set net to eval\n",
    "    model.eval()\n",
    "    \n",
    "    # loss\n",
    "    if args.dist_loss_act == 'softmax': \n",
    "        dist_loss_act = nn.Softmax(dim=1)\n",
    "    else:\n",
    "        dist_loss_act = nn.Softmax(dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            # prepare target_onehot\n",
    "            bs = np.shape(target)[0]\n",
    "            target_onehot = np.zeros(shape = (bs, args.num_classes), dtype=np.int)\n",
    "            for i in range(bs): target_onehot[i,target[i]] = 1\n",
    "            target_onehot = torch.from_numpy(target_onehot)\n",
    "            target_onehot = target_onehot.float()\n",
    "            \n",
    "            # indices for combined classes\n",
    "            class_indices = torch.LongTensor(np.concatenate((class_old, class_novel), axis=0))\n",
    "\n",
    "            # send image and target to cuda\n",
    "            if args.cuda:\n",
    "                data = data.cuda()\n",
    "                target_onehot = target_onehot.cuda()\n",
    "                class_indices = class_indices.cuda()\n",
    "                dist_loss_act = dist_loss_act.cuda()\n",
    "\n",
    "            # predict output\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate output and target one_hot\n",
    "            output = torch.index_select(output, 1, class_indices)\n",
    "            output = dist_loss_act(output)\n",
    "            output = output.cpu().data.numpy()\n",
    "            target_onehot = torch.index_select(target_onehot, 1, class_indices)\n",
    "            #target_onehot = target_onehot[:, np.concatenate((class_old, class_novel), axis=0)]\n",
    "\n",
    "            # calculation accuracy\n",
    "            acc = np.sum(np.equal(np.argmax(output, axis=-1), np.argmax(target_onehot.cpu().data.numpy(), axis=-1)))\n",
    "            acc_avg += acc\n",
    "            num_exp += np.shape(target)[0]\n",
    "\n",
    "    # calculate average accuracy\n",
    "    acc_avg /= num_exp\n",
    "            \n",
    "    # time calculation\n",
    "    tend = time.clock()\n",
    "    tcost = tend - tstart\n",
    "\n",
    "    return(tcost, acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def train (args, period, model, model_old, train_loader, loss_criterion, \\\n",
    "           optimizer, class_old, class_novel, finetune):\n",
    "    '''\n",
    "    arguments: period, net, net_old, train_loader, loss_criterion, loss_activation, optimizer, clss_old, class_novel, finetune\n",
    "    returns: tcost, loss_avg, acc_avg\n",
    "    '''\n",
    "\n",
    "    acc_avg = 0\n",
    "    num_exp = 0\n",
    "    loss_avg = 0\n",
    "    loss_cls_avg = 0\n",
    "    loss_dist_avg = 0\n",
    "    tstart = time.clock()\n",
    "\n",
    "    # set net to train mode\n",
    "    model.train()\n",
    "    model_old.train()\n",
    "    \n",
    "    # distillation loss activation\n",
    "    if not args.use_SC and args.dist_loss_act == 'softmax': \n",
    "        dist_loss_act = nn.Softmax(dim=1)\n",
    "        if args.cuda: dist_loss_act = dist_loss_act.cuda()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        bs = np.shape(target)[0]\n",
    "        \n",
    "        if not args.use_SC:\n",
    "            # prepare target_onehot    \n",
    "            target_onehot = np.zeros(shape = (bs, args.num_classes), dtype=np.int)\n",
    "            for i in range(bs): target_onehot[i,target[i]] = 1\n",
    "            target_onehot = torch.from_numpy(target_onehot)\n",
    "            target_onehot = target_onehot.float()\n",
    "\n",
    "            # indices for combined classes\n",
    "            class_indices = torch.LongTensor(np.concatenate((class_old, class_novel), axis=0))\n",
    "        \n",
    "        else:\n",
    "            data = torch.cat([data[0], data[1]], dim=0) # torch.Size([40, 3, 32, 32])  when batch_size = 20\n",
    "        # send data to cuda\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "            if args.use_SC:\n",
    "                target = target.cuda()\n",
    "            else:\n",
    "                target_onehot = target_onehot.cuda()\n",
    "                class_indices = class_indices.cuda()\n",
    "\n",
    "        # predict output\n",
    "        output = model(data)\n",
    "        \n",
    "        if args.use_SC:\n",
    "            f1, f2 = torch.split(output, [bs, bs], dim=0)\n",
    "            features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1) # `features` needs to be [bsz, n_views, ...], at least 3 dimensions are required.\n",
    "            combined_loss = loss_criterion(features, target)\n",
    "        else:\n",
    "            # loss for network\n",
    "            output_new_onehot = torch.index_select(output, 1, class_indices)\n",
    "            target_onehot = torch.index_select(target_onehot, 1, class_indices)\n",
    "            combined_loss = loss_criterion(output_new_onehot, target_onehot)\n",
    "\n",
    "        ''' ===== Distillation loss based on old net ====='''\n",
    "        if (period > 0):\n",
    "            \n",
    "            if not args.use_SC:\n",
    "                # indices of old class\n",
    "                if not finetune:\n",
    "                    class_indices = torch.LongTensor(class_old)\n",
    "                    if args.cuda: class_indices = class_indices.cuda()\n",
    "                    \n",
    "                # current_network output\n",
    "                dist = torch.index_select(output, 1, class_indices)\n",
    "                if args.use_ts: dist = dist/args.tscale\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # old network output\n",
    "                output_old = model_old(data)\n",
    "            \n",
    "            if args.use_SC:\n",
    "                f1_old, f2_old =  torch.split(output_old, [bs, bs], dim=0)\n",
    "                features_old_new_f1 = torch.cat([f1_old.unsqueeze(1), f1.unsqueeze(1)], dim=1)\n",
    "                features_old_new_f2 = torch.cat([f2_old.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "                loss_dist = (loss_criterion(features_old_new_f1, target) + loss_criterion(features_old_new_f2, target))/2\n",
    "        \n",
    "            else:\n",
    "                output_old = torch.index_select(output_old, 1, class_indices)\n",
    "                target_dist = Variable(output_old)\n",
    "                if args.use_ts: target_dist = target_dist/args.tscale\n",
    "            \n",
    "                if(args.dist_loss == 'ce'):\n",
    "                    loss_dist = F.binary_cross_entropy(dist_loss_act(dist), dist_loss_act(target_dist))\n",
    "                else: loss_dist = 0.0\n",
    "            \n",
    "        else: loss_dist = 0.0\n",
    "        '''----------------------------------------------'''\n",
    "\n",
    "        # loss calculatoin\n",
    "        loss = combined_loss + args.dist_ratio*loss_dist\n",
    "        loss_avg += loss.item()\n",
    "        loss_cls_avg += combined_loss.item()\n",
    "        if period == 0: loss_dist_avg += 0\n",
    "        else:loss_dist_avg += loss_dist.item()\n",
    "\n",
    "        if not args.use_SC:\n",
    "            acc = np.sum(np.equal(np.argmax(output_new_onehot.cpu().data.numpy(), axis=-1), np.argmax(target_onehot.cpu().data.numpy(), axis=-1)))\n",
    "            acc_avg += acc\n",
    "        \n",
    "        num_exp += np.shape(target)[0]\n",
    "\n",
    "        #optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # average calculation\n",
    "    loss_avg /= num_exp\n",
    "    loss_cls_avg /= num_exp\n",
    "    loss_dist_avg /= num_exp\n",
    "        \n",
    "    # average calculation\n",
    "    acc_avg /= num_exp\n",
    "\n",
    "    # time calculation\n",
    "    tend = time.clock()\n",
    "    tcost = tend - tstart\n",
    "\n",
    "    return(tcost, loss_avg, acc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Project         : Incremental learning for surgical instrument classification and feature extraction\n",
    "Lab             : MMLAB, National University of Singapore\n",
    "contributors    : Mobarak, lalith, mengya\n",
    "Note            : Lable smoothing loss, code adopted from our previous work and modified.\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CELossWithLS(torch.nn.Module):\n",
    "    def __init__(self, smoothing=0.1, gamma=3.0, isCos=True, ignore_index=-1):\n",
    "        super(CELossWithLS, self).__init__()\n",
    "        self.complement = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        with torch.no_grad():\n",
    "            smoothen_ohlabel = target * self.complement + self.smoothing / target.shape[1]\n",
    "        \n",
    "        target_labels = torch.argmax(target, dim=1)\n",
    "        #print(target_labels)\n",
    "        logs = self.log_softmax(logits[target_labels!=self.ignore_index])\n",
    "        pt = torch.exp(logs)\n",
    "        return -torch.sum((1-pt).pow(self.gamma)*logs * smoothen_ohlabel[target_labels!=self.ignore_index], dim=1).mean()\n",
    "    \n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):  # in main_supcon.py, mask=None\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "        # features: [bsz, n_views, f_dim]\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)   # labels,  the shape is (batch_size,1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.t()).float().to(device) # D.T --> D.t()  torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1] # contrast_count is the number of views\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.t()), # contrast_feature.T\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()   # Zi * Zj/t\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count) # mask.repeat(2,2)\n",
    "        # mask-out self-contrast    (mask the case of i=j) torch.scatter(input, dim, index, src)\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        ) \n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def save_model(args, best_model):\n",
    "    '''\n",
    "    save model\n",
    "    '''\n",
    "    if not os.path.exists('./weights'): os.mkdir('weights/')\n",
    "    \n",
    "    filename = os.path.join('weights', args.log_name + '_model.tar')\n",
    "    torch.save(best_model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    print('ls', args.use_ls, 'ts', args.use_ts, 'cbs', args.use_cbs, 'SuperCon', args.use_SC)\n",
    "    \n",
    "    # learning rate schedules\n",
    "    schedules = range(args.schedule_interval, args.epoch_base, args.schedule_interval)\n",
    "\n",
    "    # class order in icremental learning\n",
    "    class_order = np.arange(args.num_classes) #np.random.permutation(args.num_class)\n",
    "    print('class order:', class_order)\n",
    "\n",
    "    # check for pre-trained model\n",
    "    model_path = args.checkpointfile + '_%d_%s%s' % (0, ''.join(str(e) for e in class_order[args.num_class_novel[0]:args.num_class_novel[1]]), '.pkl')\n",
    "    flag_model = os.path.exists(model_path)\n",
    "\n",
    "    # network\n",
    "    if args.use_SC: model = SupConResNet(name=args.model, args=args)\n",
    "    else: model = ResNet18(args)\n",
    "    model_old = copy.deepcopy(model)\n",
    "    \n",
    "    # curicullum learning\n",
    "    if args.use_cbs:\n",
    "        if args.use_SC:\n",
    "            model.encoder.get_new_kernels(0)\n",
    "            model_old.encoder.get_new_kernels(0)  \n",
    "        else:\n",
    "            model.get_new_kernels(0)\n",
    "            model_old.get_new_kernels(0)  \n",
    "    \n",
    "    # loss\n",
    "    if args.use_SC:\n",
    "        loss_criterion = SupConLoss(temperature=args.SC_temp)\n",
    "    elif args.use_ls: \n",
    "        loss_criterion = CELossWithLS(smoothing = 0.1, gamma=0.0, isCos=False, ignore_index=-1)\n",
    "    else: \n",
    "        loss_criterion = CELossWithLS(smoothing= 0.0, gamma=0.0, isCos=False, ignore_index=-1)\n",
    "    \n",
    "    # gpu\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    if num_gpu > 0:\n",
    "        device_ids = np.arange(num_gpu).tolist()\n",
    "        print('device_ids:', device_ids)\n",
    "        if args.use_SC:\n",
    "            model.encoder = torch.nn.DataParallel(model.encoder)\n",
    "            model_old.encoder = torch.nn.DataParallel(model_old.encoder)\n",
    "            model = model.cuda()\n",
    "            model_old = model_old.cuda()\n",
    "        else:\n",
    "            model = nn.DataParallel(model, device_ids=device_ids).cuda()\n",
    "            model_old = nn.DataParallel(model_old, device_ids=device_ids).cuda()\n",
    "        loss_criterion = loss_criterion.cuda()\n",
    "    else: print('only cpu is available')\n",
    "        \n",
    "    if args.use_SC:\n",
    "        # transformer for SC\n",
    "        transform = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "                    transforms.ToTensor(),\n",
    "                    ])\n",
    "        train_transform = TwoCropTransform(transform)\n",
    "    else: train_transform = None\n",
    "    \n",
    "    # initializing classes, accuracy and memory array\n",
    "    memory_train = []                                  # train memory array\n",
    "    class_old = np.array([], dtype=int)                # old class array\n",
    "    acc_nvld_basic = np.zeros((args.period_train))     # accuracy list\n",
    "    acc_nvld_finetune = np.zeros((args.period_train))  # accuracy list\n",
    "\n",
    "    \n",
    "    for period in range(args.period_train):\n",
    "\n",
    "        print('===================== period = %d ========================='%(period))\n",
    "\n",
    "        # current 10 classes\n",
    "        class_novel = class_order[args.num_class_novel[period]:args.num_class_novel[period+1]]\n",
    "        print('class_novel:', class_novel)\n",
    "\n",
    "        # combined train dataloader\n",
    "        combined_train_files = memory_train + args.novel_train_files[period:period+1]\n",
    "        combined_train_dataset = SurgicalClassDataset18_incremental(filenames= combined_train_files, transform = train_transform, is_train=True)\n",
    "        combined_train_loader = DataLoader(dataset=combined_train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
    "        print('train files: \\t size: ', len(combined_train_loader.dataset), ' , files: ', combined_train_files)\n",
    "        \n",
    "        if not args.use_SC:\n",
    "            # test dataloader\n",
    "            combined_test_files = args.novel_test_files[0:period+1]\n",
    "            test_dataset = SurgicalClassDataset18_incremental(filenames= combined_test_files, is_train=False)\n",
    "            test_loader = DataLoader(dataset=test_dataset, batch_size= args.batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
    "            print('train files: \\t size: ', len(test_loader.dataset), ' , files: ', combined_test_files)\n",
    "\n",
    "        # initialize variables\n",
    "        lrc = args.lr\n",
    "        acc_training = []\n",
    "        print('current lr = %f' % (lrc))\n",
    "\n",
    "        # epoch training\n",
    "        for epoch in range(args.epoch_base):\n",
    "        \n",
    "            # load pretrained model\n",
    "            if period == 0 and flag_model:\n",
    "                print('load model: %s' % model_path)\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "            \n",
    "            if args.use_cbs:\n",
    "                if args.use_SC:\n",
    "                    model.encoder.module.get_new_kernels(epoch)\n",
    "                    model_old.encoder.module.get_new_kernels(epoch)\n",
    "                else:\n",
    "                    model.module.get_new_kernels(epoch)\n",
    "                    model_old.module.get_new_kernels(epoch)\n",
    "                model.cuda()\n",
    "                model_old.cuda()\n",
    "\n",
    "            ''' ====== training combined ======''' \n",
    "            # decaying learning rate\n",
    "            if epoch in schedules:\n",
    "                lrc *= args.gamma\n",
    "                print('current lr = %f' % (lrc))\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lrc, momentum=args.momentum, weight_decay=args.decay)\n",
    "\n",
    "            # train\n",
    "            tcost, loss_avg, acc_avg = train(args, period, model, model_old, combined_train_loader, \n",
    "                                             loss_criterion, optimizer, class_old, class_novel, False)\n",
    "\n",
    "            acc_training.append(acc_avg)\n",
    "            print('Training Period: %d \\t Epoch: %d \\t time = %.1f \\t loss = %.6f \\t acc = %.4f' % (period, epoch, tcost, loss_avg, acc_avg))\n",
    "            '''--------------------------------'''\n",
    "\n",
    "            if not args.use_SC:\n",
    "                ''' ====== Test combined ======'''\n",
    "                # test model\n",
    "                tcost, acc_avg = test(args, model, test_loader,class_old, class_novel)\n",
    "\n",
    "                acc_nvld_basic[period] = acc_avg\n",
    "                print('Test(n&o)Period: %d \\t Epoch: %d \\t time = %.1f \\t\\t\\t\\t acc = %.4f' % (period, epoch, tcost, acc_avg))\n",
    "\n",
    "                # exit if pre-trained model / loss converged\n",
    "                if period == 0 and flag_model: break\n",
    "                if len(acc_training)>20 and acc_training[-1]>args.stop_acc and acc_training[-5]>args.stop_acc:\n",
    "                    print('training loss converged')\n",
    "                    break\n",
    "                '''----------------------------'''\n",
    "\n",
    "        ''' copy net-old for finetuning '''\n",
    "        model_old = copy.deepcopy(model)\n",
    "        '''-----------------------------'''\n",
    "\n",
    "        ''' ===== Finetuning ====='''\n",
    "        if period > 0:\n",
    "            \n",
    "            acc_finetune_train = []\n",
    "            lrc = args.lr*args.ft_lr_factor # finetune lr\n",
    "            print('finetune current lr = %f' % (lrc))\n",
    "\n",
    "            for epoch in range(args.epoch_finetune):\n",
    "                \n",
    "                # fine tune train_dataloaders\n",
    "                ft_size = (args.num_class_novel[period+1]-args.num_class_novel[period])*args.memory_size\n",
    "                ft_combined_train_dataset = SurgicalClassDataset18_incremental(filenames= combined_train_files, fine_tune_size = ft_size, transform=train_transform, is_train=True)\n",
    "                ft_combined_train_loader = DataLoader(dataset=ft_combined_train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
    "                if(epoch == 0):  print('finetune train size:', len(ft_combined_train_loader.dataset))\n",
    "\n",
    "                ''' ===== training combined =====''' \n",
    "                # learning rate\n",
    "                if epoch in schedules:\n",
    "                    lrc *= args.gamma\n",
    "                    print('current lr = %f'%(lrc))\n",
    "\n",
    "                # optimizer\n",
    "                # criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=lrc, momentum=args.momentum, weight_decay=args.decay)\n",
    "\n",
    "                # train\n",
    "                tcost, loss_avg, acc_avg = train(args, period, model, model_old, ft_combined_train_loader, \n",
    "                                                 loss_criterion, optimizer, class_old, class_novel, True)\n",
    "\n",
    "                acc_finetune_train.append(acc_avg)\n",
    "                print('Finetune Training Period: %d \\t Epoch: %d \\t time = %.1f \\t loss = %.6f \\t acc = %.4f'%(period, epoch, tcost, loss_avg, acc_avg))\n",
    "                '''------------------------------'''\n",
    "\n",
    "                if not args.use_SC:\n",
    "                    ''' ===== Test combined ====='''\n",
    "                    # test\n",
    "                    tcost, acc_avg = test(args, model, test_loader, class_old, class_novel)\n",
    "\n",
    "                    acc_nvld_finetune[period] = acc_avg\n",
    "                    print('Finetune Test(n&o) Period: %d \\t Epoch: %d \\t time = %.1f \\t\\t\\t\\t acc = %.4f' % (period, epoch, tcost, acc_avg))\n",
    "\n",
    "                    if len(acc_finetune_train) > 20 and acc_finetune_train[-1] > args.stop_acc and acc_finetune_train[-5] > args.stop_acc:\n",
    "                        print('finetune training loss converged')\n",
    "                        break\n",
    "                    '''--------------------------'''\n",
    "                \n",
    "            print('------------------- result ------------------------')\n",
    "            print('Period: %d, basic acc = %.4f, finetune acc = %.4f' % (period, acc_nvld_basic[period], acc_nvld_finetune[period]))\n",
    "            print('---------------------------------------------------')\n",
    "\n",
    "        if period == args.period_train-1:\n",
    "            print('------------------- ave result ------------------------')\n",
    "            print('basic acc = %.4f, finetune acc = %.4f' % (np.mean(acc_nvld_basic[1:], keepdims=False), np.mean(acc_nvld_finetune[1:], keepdims=False)))\n",
    "            print('---------------------------------------------------')\n",
    "\n",
    "        print('===========================================================')\n",
    "\n",
    "        # save model\n",
    "        model_path = args.checkpointfile + '_%d_%s%s' % (0, ''.join(str(e) for e in class_order[args.num_class_novel[0]:args.num_class_novel[period+1]]), '.pkl')\n",
    "        print('save model: %s' % model_path)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        ''' ===== random images selection ====='''\n",
    "        #remove memory files from old runs\n",
    "        if os.path.exists(('data_files/memory_'+str(period)+'.txt')): \n",
    "            os.remove(('data_files/memory_'+str(period)+'.txt'))\n",
    "\n",
    "        curr_file = open((args.novel_train_files[period]), 'r') \n",
    "        memory_file = open(('data_files/memory_'+str(period)+'.txt'), 'a')\n",
    "        Lines = curr_file.readlines()        \n",
    "        indices = np.random.permutation(len(Lines))\n",
    "        Lines = [Lines[i] for i in indices[0:((args.num_class_novel[period+1]-args.num_class_novel[period])*args.memory_size)]]\n",
    "        for line in Lines: memory_file.write(line)\n",
    "        curr_file.close()\n",
    "        memory_file.close()\n",
    "\n",
    "        # add new memory file to memory train list\n",
    "        memory_train.append('data_files/memory_'+str(period)+'.txt')\n",
    "        print('memory_train', memory_train)\n",
    "        '''------------------------------------'''\n",
    "\n",
    "        #append new class images (create new)\n",
    "        class_old = np.append(class_old, class_novel, axis=0)\n",
    "    \n",
    "    print('acc_base    : ', acc_nvld_basic)     # accuracy list\n",
    "    print('acc_finetune: ', acc_nvld_finetune)\n",
    "\n",
    "    print('xxx')\n",
    "    \n",
    "#     if args.save_model:\n",
    "#         print('saving_model')\n",
    "#         save_model(args, best_model)\n",
    "#     else: print('save is disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls False ts False cbs True SuperCon True\n",
      "class order: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "device_ids: [0, 1, 2]\n",
      "===================== period = 0 =========================\n",
      "class_novel: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "train files: \t size:  7019  , files:  ['data_files/class0_10_train.txt']\n",
      "current lr = 0.001000\n",
      "Training Period: 0 \t Epoch: 0 \t time = 378.4 \t loss = 0.170618 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 1 \t time = 379.1 \t loss = 0.161728 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 2 \t time = 379.3 \t loss = 0.155931 \t acc = 0.0000\n",
      "current lr = 0.000800\n",
      "Training Period: 0 \t Epoch: 3 \t time = 379.6 \t loss = 0.150638 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 4 \t time = 378.5 \t loss = 0.148365 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 5 \t time = 379.8 \t loss = 0.145825 \t acc = 0.0000\n",
      "current lr = 0.000640\n",
      "Training Period: 0 \t Epoch: 6 \t time = 380.5 \t loss = 0.142976 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 7 \t time = 380.8 \t loss = 0.140931 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 8 \t time = 380.8 \t loss = 0.139727 \t acc = 0.0000\n",
      "current lr = 0.000512\n",
      "Training Period: 0 \t Epoch: 9 \t time = 381.6 \t loss = 0.137903 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 10 \t time = 381.4 \t loss = 0.136850 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 11 \t time = 380.6 \t loss = 0.135059 \t acc = 0.0000\n",
      "current lr = 0.000410\n",
      "Training Period: 0 \t Epoch: 12 \t time = 380.9 \t loss = 0.133840 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 13 \t time = 381.4 \t loss = 0.132659 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 14 \t time = 381.6 \t loss = 0.131809 \t acc = 0.0000\n",
      "current lr = 0.000328\n",
      "Training Period: 0 \t Epoch: 15 \t time = 382.3 \t loss = 0.131389 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 16 \t time = 381.0 \t loss = 0.130493 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 17 \t time = 380.6 \t loss = 0.129884 \t acc = 0.0000\n",
      "current lr = 0.000262\n",
      "Training Period: 0 \t Epoch: 18 \t time = 381.1 \t loss = 0.128785 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 19 \t time = 380.9 \t loss = 0.127821 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 20 \t time = 380.8 \t loss = 0.129627 \t acc = 0.0000\n",
      "current lr = 0.000210\n",
      "Training Period: 0 \t Epoch: 21 \t time = 380.2 \t loss = 0.127733 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 22 \t time = 379.9 \t loss = 0.126793 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 23 \t time = 379.7 \t loss = 0.126346 \t acc = 0.0000\n",
      "current lr = 0.000168\n",
      "Training Period: 0 \t Epoch: 24 \t time = 379.7 \t loss = 0.125215 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 25 \t time = 379.2 \t loss = 0.127840 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 26 \t time = 379.5 \t loss = 0.126165 \t acc = 0.0000\n",
      "current lr = 0.000134\n",
      "Training Period: 0 \t Epoch: 27 \t time = 379.9 \t loss = 0.125138 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 28 \t time = 379.7 \t loss = 0.124747 \t acc = 0.0000\n",
      "Training Period: 0 \t Epoch: 29 \t time = 380.1 \t loss = 0.124225 \t acc = 0.0000\n",
      "------------------- ave result ------------------------\n",
      "basic acc = nan, finetune acc = nan\n",
      "---------------------------------------------------\n",
      "===========================================================\n",
      "save model: checkpoint/incremental/ResNet18_SC_CBS_0_012345678910.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_train ['data_files/memory_0.txt']\n",
      "acc_base    :  [0.]\n",
      "acc_finetune:  [0.]\n",
      "xxx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "\n",
    "#python3 main.py --dataset cifar10 --alg res --data ./data/\n",
    "\n",
    "def seed_everything(seed=27):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # File locations \n",
    "    #novel_train_files = ['data_files/class0_8_train.txt', 'data_files/class9_10_train.txt']\n",
    "    #novel_test_files = ['data_files/class0_8_test.txt', 'data_files/class9_10_test.txt']\n",
    "    novel_train_files = ['data_files/class0_10_train.txt']\n",
    "    novel_test_files = ['data_files/class0_10_test.txt']\n",
    "    #novel_train_files = ['data_files/class0_8_train.txt']\n",
    "    #novel_test_files = ['data_files/class0_8_test.txt']\n",
    "    \n",
    "    \n",
    "    '''--------------------------------------------------- Arguments ------------------------------------------------------------'''\n",
    "    parser = argparse.ArgumentParser(description='Incremental learning for feature extraction')\n",
    "\n",
    "    # incremental learning\n",
    "    parser.add_argument('--epoch_base',         type=int,       default=30,          help='30')\n",
    "    parser.add_argument('--epoch_finetune',     type=int,       default=15,          help='15')\n",
    "    parser.add_argument('--batch_size',         type=int,       default=20,          help='20')\n",
    "    parser.add_argument('--period_train',       type=int,       default=1,           help='2')\n",
    "    parser.add_argument('--num_classes',        type=int,       default=11,          help='11')\n",
    "    parser.add_argument('--num_class_novel',                    default=[0,11],    help='[0,9,11]')\n",
    "    parser.add_argument('--memory_size',                        default=50,          help='50')\n",
    "\n",
    "    parser.add_argument('--stop_acc',           type=float,     default=0.998,       help='number of epochs')\n",
    "\n",
    "    # datasets\n",
    "    parser.add_argument('--novel_train_files',  default=novel_train_files,           help='list of train files')\n",
    "    parser.add_argument('--novel_test_files',   default=novel_test_files,            help='list of test files')\n",
    "\n",
    "    # learning rate\n",
    "    parser.add_argument('--schedule_interval',  type=int,       default=3,           help='decay epoch rate: 3')\n",
    "    parser.add_argument('--lr',                 type=float,     default=0.001,       help='learn rate: 0.001') \n",
    "    parser.add_argument('--gamma',              type=float,     default=0.8,         help='decay lr factor: 0.8')\n",
    "    parser.add_argument('--ft_lr_factor',       type=float,     default=0.1,         help='ft learn rate: 0.1')\n",
    "    \n",
    "    # loss\n",
    "    parser.add_argument('--dist_loss',          type=str,       default='ce',        help='dist_loss')\n",
    "    parser.add_argument('--dist_loss_act',      type=str,       default='softmax',   help='dist_loss_act')\n",
    "    parser.add_argument('--dist_ratio',         type=float,     default=0.5,         help='dist_loss_ratio')\n",
    "    \n",
    "    # optimizer\n",
    "    parser.add_argument('--momentum',           type=float,     default=0.6,         help='learning momentum') \n",
    "    parser.add_argument('--decay',              type=float,     default=0.0001,      help='learning rate')\n",
    "    \n",
    "    # Label smoothing\n",
    "    parser.add_argument('--use_ls',             type=bool,      default=False,        help='list of test files')\n",
    "\n",
    "    # Temperature scaling\n",
    "    parser.add_argument('--use_ts',             type=bool,      default=False,       help='use temp_scale')\n",
    "    parser.add_argument('--tscale',             type=float,     default=3.0,         help='Temp scaling')\n",
    "    \n",
    "    # CBS ARGS\n",
    "    parser.add_argument('--use_cbs',            type=bool,      default=True,       help='use CBS')\n",
    "    parser.add_argument('--std',                type=float,     default=1.0,         help='')\n",
    "    parser.add_argument('--std_factor',         type=float,     default=0.9,         help='')\n",
    "    parser.add_argument('--cbs_epoch',          type=int,       default=5,           help='')\n",
    "    parser.add_argument('--kernel_size',        type=int,       default=3,           help='')\n",
    "    parser.add_argument('--fil1',               type=str,       default='LOG',       help='gau, LOG')\n",
    "    parser.add_argument('--fil2',               type=str,       default='gau',       help='gau, LOG')\n",
    "    parser.add_argument('--fil3',               type=str,       default='gau',       help='gau, LOG')\n",
    "    \n",
    "    # SupCon ARGS\n",
    "    parser.add_argument('--use_SC',             type=bool,      default=True,       help='use SuperCon')\n",
    "    #parser.add_argument('--save_freq',   type=int, default=10, help='save frequency')\n",
    "    parser.add_argument('--model',              type=str,       default='resnet18')\n",
    "    parser.add_argument('--size',               type=int,       default=224,        help='Random crop size') # 32    \n",
    "    parser.add_argument('--SC_temp',            type=float,     default=0.07,       help='temperature for loss function')  \n",
    "    \n",
    "    \n",
    "    \n",
    "    parser.add_argument('--save_model',         type=bool,      default=False,       help='store_true')\n",
    "    parser.add_argument('--checkpointfile',     type=str,       default='checkpoint/incremental/ResNet18_SC_CBS')\n",
    "   \n",
    "    args = parser.parse_args(args=[])\n",
    "    '''-------------------------------------------------------------------------------------------------------------------------'''\n",
    "    \n",
    "    if torch.cuda.is_available(): args.cuda = True\n",
    "    \n",
    "    seed_everything()\n",
    "    main(args)\n",
    "\n",
    "#python3 main.py --dataset cifar10 --alg res --data ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "if args.use_ts: print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch count: 0 \t accuracy: 42.86\n",
    "best acc: 0 \t best acc: 42.86\n",
    "epoch count: 1 \t accuracy: 48.08\n",
    "best acc: 1 \t best acc: 48.08\n",
    "epoch count: 2 \t accuracy: 50.50\n",
    "best acc: 2 \t best acc: 50.50\n",
    "epoch count: 3 \t accuracy: 57.78\n",
    "best acc: 3 \t best acc: 57.78\n",
    "epoch count: 4 \t accuracy: 58.14\n",
    "best acc: 4 \t best acc: 58.14\n",
    "epoch count: 5 \t accuracy: 63.66\n",
    "best acc: 5 \t best acc: 63.66\n",
    "epoch count: 6 \t accuracy: 63.52\n",
    "best acc: 5 \t best acc: 63.66\n",
    "epoch count: 7 \t accuracy: 62.68\n",
    "best acc: 5 \t best acc: 63.66\n",
    "epoch count: 8 \t accuracy: 67.59\n",
    "best acc: 8 \t best acc: 67.59\n",
    "epoch count: 9 \t accuracy: 62.43\n",
    "best acc: 8 \t best acc: 67.59\n",
    "epoch count: 10 \t accuracy: 68.83\n",
    "best acc: 10 \t best acc: 68.83\n",
    "epoch count: 11 \t accuracy: 70.76\n",
    "best acc: 11 \t best acc: 70.76\n",
    "epoch count: 12 \t accuracy: 70.52\n",
    "best acc: 11 \t best acc: 70.76\n",
    "epoch count: 13 \t accuracy: 66.54\n",
    "best acc: 11 \t best acc: 70.76\n",
    "epoch count: 14 \t accuracy: 67.69\n",
    "best acc: 11 \t best acc: 70.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
