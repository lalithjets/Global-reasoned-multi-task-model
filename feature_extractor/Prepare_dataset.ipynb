{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================= Create Train and Test Data ================= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "if sys.version_info[0] == 2:\n",
    "    import xml.etree.cElementTree as ET\n",
    "else:\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "dataset = 'Youtube'\n",
    "\n",
    "if dataset == 'miccai18':\n",
    "    \n",
    "    # seq_list\n",
    "    mlist      = [1,2,3,4,5,6,7,9,10,11,12,14,15,16]\n",
    "    seq_list   = [1,2,3,4,5,6,7,9,10,11,12,14,15,16]\n",
    "    train_list = [2,3,4,6,7,9,10,11,12,14,15]\n",
    "    \n",
    "    # img location\n",
    "    dir_root_gt = 'instruments18/seq_'\n",
    "    img_folder  = 'left_frames/'\n",
    "    file_format = '.png'\n",
    "    \n",
    "    # destination location\n",
    "    train_folder = 'Classification_dataset/train_new/'\n",
    "    test_folder  = 'Classification_dataset/test_new/'\n",
    "    \n",
    "    INSTRUMENT_CLASSES = ('kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "\n",
    "    myfile = open('miccai_targets.txt', 'w')\n",
    "    \n",
    "elif dataset == 'miccai17':\n",
    "    \n",
    "    # seq_list\n",
    "    mlist      = [ 1, 2, 3, 4, 5, 6, 7, 8, 9,10]\n",
    "    seq_list   = [17,18,19,20,21,22,23,24,25,26]\n",
    "    train_list = [6,10]\n",
    "    \n",
    "    # img location\n",
    "    dir_root_gt = 'instruments17/instrument_dataset_'\n",
    "    img_folder  = 'images/'\n",
    "    file_format = '.jpg'\n",
    "    \n",
    "    # destination location\n",
    "    train_folder = 'Classification_dataset/train_new/'\n",
    "    test_folder  = 'Classification_dataset/test_new/'\n",
    "    \n",
    "    INSTRUMENT_CLASSES = ('tissue', 'Bipolar Forceps', 'Prograsp Forceps', 'Large Needle Driver',\n",
    "                      'Monopolar Curved Scissors', 'Others', 'Suction', 'Clip Applier',\n",
    "                      'Stapler', 'Maryland Dissector', 'Spatulated Monopolar Cautery',\n",
    "                      'Vessel Sealer', 'Grasping Retractor')\n",
    "\n",
    "    myfile = open('miccai17_targets.txt', 'w')\n",
    "    \n",
    "elif dataset == 'SGH_2020':\n",
    "    \n",
    "    # seq_list\n",
    "    mlist      = [ 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18,19,20,21,22]\n",
    "    seq_list   = [27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48]\n",
    "    train_list = [ 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15]\n",
    "    \n",
    "    # img location\n",
    "    dir_root_gt = 'SGH_dataset_2020/'\n",
    "    img_folder  = 'resized_frames/'\n",
    "    file_format = '.png'\n",
    "    \n",
    "    # destination location\n",
    "    train_folder = 'Classification_dataset/train_new/'\n",
    "    test_folder  = 'Classification_dataset/test_new/'\n",
    "    \n",
    "    INSTRUMENT_CLASSES = ('tissue', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "\n",
    "    myfile = open('sgh_2020_targets.txt', 'w')\n",
    "    \n",
    "elif dataset == 'Youtube':\n",
    "    \n",
    "    # seq_list\n",
    "    mlist      = [ 1, 2, 3, 4, 5]\n",
    "    seq_list   = [49,50,51,52,53]\n",
    "    train_list = [5]\n",
    "    \n",
    "    # img location\n",
    "    dir_root_gt = 'YouTubeDataset/'\n",
    "    img_folder  = 'resized_frames/'\n",
    "    file_format = '.png'\n",
    "    \n",
    "    # destination location\n",
    "    train_folder = 'Classification_dataset/train_new/'\n",
    "    test_folder  = 'Classification_dataset/test_new/'\n",
    "    \n",
    "    INSTRUMENT_CLASSES = ('tissue', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "\n",
    "    myfile = open('youtube_targets.txt', 'w')\n",
    "\n",
    "xml_dir_list = []\n",
    "\n",
    "for index, ind_seq in tqdm(enumerate(mlist)):\n",
    "    xml_dir_temp = dir_root_gt + str(ind_seq) + '/xml/'\n",
    "    xml_dir_list = glob(xml_dir_temp + '/*.xml')\n",
    "    for xml_dir_temp in xml_dir_list:\n",
    "        file_name = os.path.splitext(os.path.basename(xml_dir_temp))[0]\n",
    "        file_root = os.path.dirname(os.path.dirname(xml_dir_temp))\n",
    "        _xml = ET.parse(xml_dir_temp).getroot()\n",
    "        _img_dir = os.path.join(file_root, img_folder, file_name + file_format)\n",
    "        _img = Image.open(_img_dir).convert('RGB')\n",
    "\n",
    "        class_to_ind = dict(zip(INSTRUMENT_CLASSES, range(len(INSTRUMENT_CLASSES))))\n",
    "        node_bbox = []\n",
    "        det_classes = []\n",
    "        for obj in _xml.iter('objects'):\n",
    "            name = obj.find('name').text.strip()\n",
    "            # interact = obj.find('interaction').text.strip()\n",
    "            det_classes.append(INSTRUMENT_CLASSES.index(str(name)))\n",
    "            bbox = obj.find('bndbox')\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            bndbox = []\n",
    "            label_idx = class_to_ind[name]\n",
    "            # interaction_idx = interaction_to_ind[interact]\n",
    "            for i, pt in enumerate(pts):\n",
    "                cur_pt = int(bbox.find(pt).text)\n",
    "                # cur_pt = cur_pt / _img_shape[1] if i % 2 == 0 else cur_pt / _img_shape[0]\n",
    "                bndbox.append(cur_pt)\n",
    "            bndbox.append(label_idx)\n",
    "            node_bbox += [bndbox]\n",
    "\n",
    "        _img = np.array(_img)\n",
    "        idx = 0\n",
    "        for bndbox in node_bbox:\n",
    "            roi = np.array(bndbox).astype(int)\n",
    "            roi_crop = _img[roi[1]:roi[3] + 1, roi[0]:roi[2] + 1,:]\n",
    "            roi_crop = Image.fromarray(roi_crop).resize((224, 224), Image.NEAREST)\n",
    "            if ind_seq in train_list:\n",
    "                roi_dir = train_folder+file_name+'_'+str(idx)+'_'+format(seq_list[index], '02d')+'_'+str(roi[4])+'.png'\n",
    "            else:\n",
    "                roi_dir = test_folder+file_name+'_'+str(idx)+'_'+format(seq_list[index], '02d')+'_'+str(roi[4])+'.png'\n",
    "            roi_crop.save(roi_dir)\n",
    "            myfile.write(\"%s %s\\n\" %(roi_dir, str(roi[4])))\n",
    "            idx += 1\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================= Count data ================= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#System\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "if sys.version_info[0] == 2: import xml.etree.cElementTree as ET\n",
    "else: import xml.etree.ElementTree as ET\n",
    "\n",
    "# input data and IO folder location\n",
    "mlist = [4]\n",
    "\n",
    "dir_root_gt = 'YouTubeDataset/'\n",
    "xml_dir_list = []\n",
    "\n",
    "for i in mlist:\n",
    "    xml_dir_temp = dir_root_gt + str(i) + '/xml/'\n",
    "    seq_list_each = glob(xml_dir_temp + '/*.xml')\n",
    "    xml_dir_list = xml_dir_list + seq_list_each\n",
    "    \n",
    "# global variables\n",
    "INSTRUMENT_CLASSES = ('tissue', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "\n",
    "ACTION_CLASSES = ('Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', 'Tool_Manipulation',\n",
    "                  'Cutting', 'Cauterization', 'Suction', 'Looping', 'Suturing', 'Clipping', \n",
    "                  'Staple', 'Ultrasound_Sensing')\n",
    "\n",
    "instrument_cls_freq = np.zeros((13,1))\n",
    "action_cls_freq = np.zeros((13,1))\n",
    "\n",
    "for index, _xml_dir in  enumerate(xml_dir_list):\n",
    "    _xml = ET.parse(_xml_dir).getroot()\n",
    "    c_flag = False\n",
    "    \n",
    "    for obj in _xml.iter('objects'):\n",
    "        # object name and interaction type\n",
    "        name = obj.find('name').text.strip()\n",
    "        #print(name)\n",
    "        #interact = obj.find('interaction').text.strip()\n",
    "        instrument_cls_freq[int(INSTRUMENT_CLASSES.index(str(name)))] += 1\n",
    "        #action_cls_freq[int(ACTION_CLASSES.index(str(interact)))] += 1\n",
    "    if c_flag: continue\n",
    "\n",
    "print('instrument', instrument_cls_freq)\n",
    "print('action', action_cls_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================= Create Train and Test Files ================= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150, 150, 150, 150, 150, 150, 150, 60, 150, 0, 0]\n",
      "[150, 150, 150, 150, 150, 150, 150, 60, 150, 100, 100]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Project         : Incremental learning for feature extraction\n",
    "Lab             : MMLAB, National University of Singapore\n",
    "contributors    : Lalith, Mengya, Mobarak\n",
    "'''\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "#period = 1\n",
    "#classes = [[0,1,2,3,4,5,6,7,8,9,10]]\n",
    "\n",
    "period = 2\n",
    "classes = [[0,1,2,3,4,5,6,7,8], [9,10]]\n",
    "\n",
    "#train\n",
    "#dir_root_gt = \"../datasets/Classification_dataset/train/*.png\"\n",
    "#file_names = ['data_files/class0_10_train.txt']\n",
    "#file_names = ['data_files/class0_8_train.txt', 'data_files/class9_10_train.txt']\n",
    "\n",
    "\n",
    "# test data: \n",
    "dir_root_gt = '../datasets/Classification_dataset/test/*'\n",
    "#file_names = ['data_files/class0_10_test.txt']\n",
    "file_names = ['data_files/class0_8_test.txt', 'data_files/class9_10_test.txt']\n",
    "\n",
    "img_list = []\n",
    "img_list = glob(dir_root_gt)\n",
    "\n",
    "class_list = [0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "\n",
    "# for every incremental learning, selects class specific images from the total image list\n",
    "for period_id in range(period):\n",
    "    curr_file= open(file_names[period_id], 'a')\n",
    "    for img in img_list:\n",
    "        target = int(img[:-4].split('_')[-1:][0])\n",
    "        if target in classes[period_id]:\n",
    "            class_list[target] += 1\n",
    "            curr_file.write(img+'\\n')\n",
    "    print(class_list)\n",
    "    curr_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
